{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c4e721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     506 non-null    float64\n",
      " 1   ZN       506 non-null    float64\n",
      " 2   INDUS    506 non-null    float64\n",
      " 3   CHAS     506 non-null    int64  \n",
      " 4   NOX      506 non-null    float64\n",
      " 5   RM       506 non-null    float64\n",
      " 6   AGE      506 non-null    float64\n",
      " 7   DIS      506 non-null    float64\n",
      " 8   RAD      506 non-null    int64  \n",
      " 9   TAX      506 non-null    float64\n",
      " 10  PTRATIO  506 non-null    float64\n",
      " 11  B        506 non-null    float64\n",
      " 12  LSTAT    506 non-null    float64\n",
      " 13  MEDV     506 non-null    float64\n",
      "dtypes: float64(12), int64(2)\n",
      "memory usage: 55.5 KB\n",
      "None\n",
      "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
      "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
      "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
      "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
      "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
      "\n",
      "   PTRATIO       B  LSTAT  MEDV  \n",
      "0     15.3  396.90   4.98  24.0  \n",
      "1     17.8  396.90   9.14  21.6  \n",
      "2     17.8  392.83   4.03  34.7  \n",
      "3     18.7  394.63   2.94  33.4  \n",
      "4     18.7  396.90   5.33  36.2  \n",
      "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
      "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
      "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
      "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
      "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
      "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
      "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
      "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
      "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
      "\n",
      "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
      "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
      "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
      "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
      "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
      "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
      "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
      "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
      "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
      "\n",
      "            LSTAT        MEDV  \n",
      "count  506.000000  506.000000  \n",
      "mean    12.653063   22.532806  \n",
      "std      7.141062    9.197104  \n",
      "min      1.730000    5.000000  \n",
      "25%      6.950000   17.025000  \n",
      "50%     11.360000   21.200000  \n",
      "75%     16.955000   25.000000  \n",
      "max     37.970000   50.000000  \n",
      "\n",
      "Missing values:\n",
      "CRIM       0\n",
      "ZN         0\n",
      "INDUS      0\n",
      "CHAS       0\n",
      "NOX        0\n",
      "RM         0\n",
      "AGE        0\n",
      "DIS        0\n",
      "RAD        0\n",
      "TAX        0\n",
      "PTRATIO    0\n",
      "B          0\n",
      "LSTAT      0\n",
      "MEDV       0\n",
      "dtype: int64\n",
      "\n",
      "Duplicated values:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Boston Housing Prices - Initial Data Inspection\n",
    "# -----------------------------------------------\n",
    "# Step 1: Load libraries and dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "\n",
    "file_path = \"/Users/jeremygoetschy/Projects/House_Prices/Raw/boston.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Dataset dimensions\n",
    "print(df.shape)\n",
    "# ðŸ”Ž Observations:\n",
    "# - The dataset contains 506 rows and 14 columns.\n",
    "\n",
    "\n",
    "# Dataset info (dtypes, nulls)\n",
    "print(df.info())\n",
    "# ðŸ”Ž Observations:\n",
    "# - All columns are numerical features.\n",
    "# - \"CHAS\" is a binary variable (Charles River dummy variable: 0 or 1).\n",
    "\n",
    "\n",
    "# Preview first few rows\n",
    "print(df.head())\n",
    "# ðŸ”Ž Observations:\n",
    "# - The \"RAD\" variable appears to be categorical (index of highway accessibility).\n",
    "# - Other features are continuous.\n",
    "\n",
    "\n",
    "# Descriptive statistics\n",
    "print(df.describe())\n",
    "# ðŸ”Ž Observations:\n",
    "# - Most features are on different scales (scaling will be required later).\n",
    "# - \"CRIM\" (per capita crime rate) is highly skewed â†’ log transformation may help.\n",
    "\n",
    "\n",
    "# Check missing values\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
    "# ðŸ”Ž Observations:\n",
    "# - No missing values detected across any column.\n",
    "\n",
    "\n",
    "# Check duplicates\n",
    "print(f\"\\nDuplicated values:\\n{df.duplicated().sum()}\")\n",
    "# ðŸ”Ž Observations:\n",
    "# - No duplicate rows found in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cafc254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAS\n",
      "0    471\n",
      "1     35\n",
      "Name: count, dtype: int64\n",
      "Outliers detected in MEDV: 40\n",
      "Outliers detected in CRIM: 66\n",
      "Outliers detected in LSTAT: 7\n",
      "\n",
      "--- Key Insights ---\n",
      "1. Target (MEDV) is capped at 50, limiting predictive accuracy at the top end.\n",
      "2. Strong predictors: RM (positive) and LSTAT (negative).\n",
      "3. High multicollinearity detected: RAD-TAX, NOX-INDUS-AGE-DIS-TAX cluster.\n",
      "4. Significant outliers in CRIM and MEDV â†’ consider log transform / robust methods.\n",
      "5. CHAS is highly imbalanced and may have limited predictive power.\n"
     ]
    }
   ],
   "source": [
    "# Boston Housing Prices - Exploratory Data Analysis (EDA)\n",
    "# -------------------------------------------------------\n",
    "# Goal: Explore the distribution of features, relationships with the target (MEDV),\n",
    "#       and identify outliers/correlations.\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\", color_codes=True)\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 1. Target Variable (MEDV)\n",
    "# ============================\n",
    "plt.figure()\n",
    "sns.histplot(df[\"MEDV\"], bins=30, kde=True)\n",
    "plt.title(\"Distribution of MEDV\")\n",
    "plt.xlabel(\"Median Value of Owner-Occupied Homes ($1000s)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"figures/medv_distribution.png\")\n",
    "plt.close()\n",
    "\n",
    "# ðŸ”Ž Observations:\n",
    "# - Target (MEDV) shows a normal-like distribution with some skew.\n",
    "# - Prices are **capped at 50** â†’ artificial ceiling in dataset.\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 2. Feature Distributions\n",
    "# ============================\n",
    "features = [col for col in df.columns if col not in [\"MEDV\", \"CHAS\"]]\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, feature in enumerate(features, 1):\n",
    "    plt.subplot(4, 3, i)\n",
    "    sns.histplot(df[feature], bins=30, kde=True)\n",
    "    plt.title(f\"Distribution of {feature}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/feature_distributions.png\")\n",
    "plt.close()\n",
    "\n",
    "# ðŸ”Ž Observations:\n",
    "# - CRIM (crime rate) is highly skewed â†’ potential log transform candidate.\n",
    "# - LSTAT also shows skewness.\n",
    "# - Most features are continuous, except \"RAD\" which behaves like categorical.\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 3. Boxplots for Outlier Detection\n",
    "# ============================\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i, feature in enumerate(features, 1):\n",
    "    plt.subplot(4, 3, i)\n",
    "    sns.boxplot(y=df[feature])\n",
    "    plt.title(f\"Boxplot of {feature}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/feature_boxplots.png\")\n",
    "plt.close()\n",
    "\n",
    "# ðŸ”Ž Observations:\n",
    "# - Strong outliers detected in CRIM, ZN, and MEDV.\n",
    "# - Moderate outliers in LSTAT.\n",
    "# - Feature \"B\" shows very high average values.\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 4. CHAS Variable (Binary)\n",
    "# ============================\n",
    "print(df[\"CHAS\"].value_counts())\n",
    "sns.countplot(x=\"CHAS\", data=df)\n",
    "plt.title(\"Count of CHAS (Near Charles River)\")\n",
    "plt.xlabel(\"CHAS\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.savefig(\"figures/chas_countplot.png\")\n",
    "plt.close()\n",
    "\n",
    "# ðŸ”Ž Observations:\n",
    "# - CHAS is highly imbalanced â†’ only 35 houses near the river (vs 471 not near).\n",
    "# - May have limited predictive power.\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 5. MEDV vs Features (Boxplots)\n",
    "# ============================\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, feature in enumerate(features, 1):\n",
    "    plt.subplot(4, 3, i)\n",
    "    sns.boxplot(x=df[feature], y=df[\"MEDV\"])\n",
    "    plt.title(f\"MEDV vs {feature}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/medv_vs_features_boxplots.png\")\n",
    "plt.close()\n",
    "\n",
    "# ðŸ”Ž Observations:\n",
    "# - RM (average rooms) shows a positive trend with MEDV.\n",
    "# - LSTAT (lower status population %) shows a negative trend with MEDV.\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 6. Correlation Matrix\n",
    "# ============================\n",
    "plt.figure()\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1, center=0)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.savefig(\"figures/correlation_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# ðŸ”Ž Observations:\n",
    "# - MEDV has strong positive correlation with RM (0.7) and strong negative with LSTAT (-0.74).\n",
    "# - RAD and TAX are highly correlated (0.91).\n",
    "# - NOX, INDUS, AGE, DIS, TAX cluster together:\n",
    "#   - Higher industry % â†’ higher NOX, older homes, higher taxes, further from center.\n",
    "#   - DIS (distance to employment centers) is negatively correlated with these.\n",
    "# - Key predictors: RM and LSTAT.\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 7. Multivariate Relationships\n",
    "# ============================\n",
    "sns.pairplot(df[[\"RM\", \"LSTAT\", \"MEDV\"]])\n",
    "plt.savefig(\"figures/pairplot_rm_lstat_medv.png\")\n",
    "plt.close()\n",
    "\n",
    "# ðŸ”Ž Observations:\n",
    "# - MEDV vs RM shows a clear linear positive relationship.\n",
    "# - MEDV vs LSTAT is nonlinear and negative.\n",
    "# - RM and LSTAT are inversely related (more rooms â†’ lower LSTAT).\n",
    "# - The artificial MEDV cap at 50 is evident.\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 8. Outlier Detection (IQR Method)\n",
    "# ============================\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    print(f\"Outliers detected in {column}: {outliers.shape[0]}\")\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "\n",
    "for col in [\"MEDV\", \"CRIM\", \"LSTAT\"]:\n",
    "    detect_outliers_iqr(df, col)\n",
    "\n",
    "# ðŸ”Ž Observations:\n",
    "# - MEDV: 40 capped values at 50.\n",
    "# - CRIM: 66 outliers â†’ log transform recommended.\n",
    "# - LSTAT: 7 mild outliers â†’ manageable.\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 9. Drop \"B\" Feature\n",
    "# ============================\n",
    "df = df.drop(columns=[\"B\"])\n",
    "\n",
    "# ðŸ”Ž Observations:\n",
    "# - \"B\" has ethical concerns (racial proxy) and weak correlation with MEDV.\n",
    "# - Removed from dataset.\n",
    "\n",
    "\n",
    "# ============================\n",
    "# EDA Summary\n",
    "# ============================\n",
    "print(\"\\n--- Key Insights ---\")\n",
    "print(\"1. Target (MEDV) is capped at 50, limiting predictive accuracy at the top end.\")\n",
    "print(\"2. Strong predictors: RM (positive) and LSTAT (negative).\")\n",
    "print(\"3. High multicollinearity detected: RAD-TAX, NOX-INDUS-AGE-DIS-TAX cluster.\")\n",
    "print(\n",
    "    \"4. Significant outliers in CRIM and MEDV â†’ consider log transform / robust methods.\"\n",
    ")\n",
    "print(\"5. CHAS is highly imbalanced and may have limited predictive power.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1673991c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CRIM        ZN     INDUS       NOX        RM       AGE       DIS  \\\n",
      "0 -0.151682  1.131306 -0.891172 -0.571650  0.000000  0.496612 -0.250765   \n",
      "1 -0.137580  0.000000 -0.243003 -0.202943 -0.394286  0.287940  0.028542   \n",
      "2 -0.137593  0.000000 -1.103520 -0.202943 -0.394286  1.323171 -0.334353   \n",
      "3 -0.134223  0.000000 -1.403308 -0.581720 -0.457143  1.069783 -0.646279   \n",
      "4 -0.110372  0.000000 -0.821356 -0.581720 -0.457143  1.271680 -0.475025   \n",
      "\n",
      "    PTRATIO     LSTAT  CHAS    TAX  \n",
      "0  0.285777 -1.339286   0.0  296.0  \n",
      "1  0.569789 -0.446429   0.0  242.0  \n",
      "2  0.569789 -0.446429   0.0  242.0  \n",
      "3  0.924391 -0.125000   0.0  222.0  \n",
      "4  0.924391 -0.125000   0.0  222.0  \n",
      "               CRIM          ZN         INDUS         NOX          RM  \\\n",
      "count  5.060000e+02  506.000000  5.060000e+02  506.000000  506.000000   \n",
      "mean   3.996951e-01    0.366495 -4.769126e-02    0.112067    0.095400   \n",
      "std    6.986721e-01    0.622752  6.616283e-01    0.531398    0.662158   \n",
      "min   -1.516822e-01    0.000000 -1.853629e+00   -0.714950   -0.874286   \n",
      "25%   -1.021184e-01    0.000000 -5.416698e-01   -0.348567   -0.508571   \n",
      "50%    9.540979e-18    0.000000  2.723516e-16    0.000000    0.000000   \n",
      "75%    8.978816e-01    1.000000  4.583302e-01    0.651433    0.491429   \n",
      "max    2.917848e+00    1.773212  1.409500e+00    1.398141    1.902857   \n",
      "\n",
      "              AGE         DIS       PTRATIO       LSTAT        CHAS  \\\n",
      "count  506.000000  506.000000  5.060000e+02  506.000000  506.000000   \n",
      "mean     0.103163   -0.181959  1.902672e-01   -0.212309    0.069170   \n",
      "std      0.952056    0.573881  6.818457e-01    0.773195    0.253994   \n",
      "min     -3.587398   -1.520897 -6.728244e-01   -2.303571    0.000000   \n",
      "25%     -0.437669   -0.662080 -3.585445e-01   -0.589286    0.000000   \n",
      "50%      0.000000    0.000000  7.199102e-17    0.000000    0.000000   \n",
      "75%      0.562331    0.337920  6.414555e-01    0.410714    0.000000   \n",
      "max      3.484417    0.458716  2.888060e+00    1.053571    1.000000   \n",
      "\n",
      "              TAX  \n",
      "count  506.000000  \n",
      "mean   408.237154  \n",
      "std    168.537116  \n",
      "min    187.000000  \n",
      "25%    279.000000  \n",
      "50%    330.000000  \n",
      "75%    666.000000  \n",
      "max    711.000000  \n"
     ]
    }
   ],
   "source": [
    "# Boston Housing Prices - Data Preprocessing\n",
    "# ------------------------------------------\n",
    "# Goal: Apply log transformations to skewed features, scale numerical variables,\n",
    "#       and prepare data for regression models.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Features and target\n",
    "X = df.drop(columns=[\"MEDV\"])\n",
    "y = df[\"MEDV\"]\n",
    "\n",
    "# ============================\n",
    "# 1. Log Transform Skewed Features\n",
    "# ============================\n",
    "\n",
    "skewed_features = [\"CRIM\", \"ZN\", \"LSTAT\"]\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "\n",
    "# ============================\n",
    "# 2. Define Feature Groups\n",
    "# ============================\n",
    "\n",
    "numeric_features = [\n",
    "    \"CRIM\",\n",
    "    \"ZN\",\n",
    "    \"INDUS\",\n",
    "    \"NOX\",\n",
    "    \"RM\",\n",
    "    \"AGE\",\n",
    "    \"DIS\",\n",
    "    \"PTRATIO\",\n",
    "    \"LSTAT\",\n",
    "]\n",
    "\n",
    "categorical_features = [\"CHAS\", \"TAX\"]\n",
    "\n",
    "# ============================\n",
    "# 3. Build Preprocessor\n",
    "# ============================\n",
    "\n",
    "# Pipeline for skewed features: log transform + robust scaling\n",
    "log_scale_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"log\", FunctionTransformer(np.log1p, validate=True)),\n",
    "        (\"scaler\", RobustScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ColumnTransformer: apply different transformations\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"log_scale\", log_scale_pipeline, skewed_features),\n",
    "        (\n",
    "            \"scaler\",\n",
    "            RobustScaler(),\n",
    "            [col for col in numeric_features if col not in skewed_features],\n",
    "        ),\n",
    "        (\"cat\", \"passthrough\", categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 4. Apply Transformation\n",
    "# ============================\n",
    "\n",
    "pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor)])\n",
    "X_transformed = pipeline.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_transformed_df = pd.DataFrame(\n",
    "    X_transformed, columns=numeric_features + categorical_features\n",
    ")\n",
    "\n",
    "print(X_transformed_df.head())\n",
    "print(X_transformed_df.describe())\n",
    "\n",
    "# ðŸ”Ž Observations:\n",
    "# - Log transformation applied to skewed features (CRIM, ZN, LSTAT).\n",
    "# - Robust scaling used for all numeric features.\n",
    "# - Categorical features (CHAS, TAX) passed through without changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7764b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Feature  MI Score\n",
      "2     INDUS  0.665229\n",
      "5       AGE  0.528778\n",
      "3       NOX  0.472905\n",
      "4        RM  0.457528\n",
      "8     LSTAT  0.444760\n",
      "0      CRIM  0.377603\n",
      "10      TAX  0.372748\n",
      "6       DIS  0.315058\n",
      "7   PTRATIO  0.294597\n",
      "1        ZN  0.154392\n",
      "9      CHAS  0.030136\n",
      "       CRIM        ZN     INDUS       NOX        RM       AGE       DIS  \\\n",
      "0 -0.151682  1.131306 -0.891172 -0.571650  0.000000  0.496612 -0.250765   \n",
      "1 -0.137580  0.000000 -0.243003 -0.202943 -0.394286  0.287940  0.028542   \n",
      "2 -0.137593  0.000000 -1.103520 -0.202943 -0.394286  1.323171 -0.334353   \n",
      "3 -0.134223  0.000000 -1.403308 -0.581720 -0.457143  1.069783 -0.646279   \n",
      "4 -0.110372  0.000000 -0.821356 -0.581720 -0.457143  1.271680 -0.475025   \n",
      "\n",
      "    PTRATIO     LSTAT  CHAS  ...  PTRATIO^2  PTRATIO LSTAT  PTRATIO CHAS  \\\n",
      "0  0.285777 -1.339286   0.0  ...   0.081668      -0.382737           0.0   \n",
      "1  0.569789 -0.446429   0.0  ...   0.324659      -0.254370           0.0   \n",
      "2  0.569789 -0.446429   0.0  ...   0.324659      -0.254370           0.0   \n",
      "3  0.924391 -0.125000   0.0  ...   0.854498      -0.115549           0.0   \n",
      "4  0.924391 -0.125000   0.0  ...   0.854498      -0.115549           0.0   \n",
      "\n",
      "   PTRATIO TAX   LSTAT^2  LSTAT CHAS   LSTAT TAX  CHAS^2  CHAS TAX    TAX^2  \n",
      "0    84.589913  1.793686        -0.0 -396.428571     0.0       0.0  87616.0  \n",
      "1   137.888869  0.199298        -0.0 -108.035714     0.0       0.0  58564.0  \n",
      "2   137.888869  0.199298        -0.0 -108.035714     0.0       0.0  58564.0  \n",
      "3   205.214766  0.015625        -0.0  -27.750000     0.0       0.0  49284.0  \n",
      "4   205.214766  0.015625        -0.0  -27.750000     0.0       0.0  49284.0  \n",
      "\n",
      "[5 rows x 77 columns]\n",
      "       CRIM        ZN     INDUS       NOX        RM       AGE       DIS  \\\n",
      "0 -0.151682  1.131306 -0.891172 -0.571650  0.000000  0.496612 -0.250765   \n",
      "1 -0.137580  0.000000 -0.243003 -0.202943 -0.394286  0.287940  0.028542   \n",
      "2 -0.137593  0.000000 -1.103520 -0.202943 -0.394286  1.323171 -0.334353   \n",
      "3 -0.134223  0.000000 -1.403308 -0.581720 -0.457143  1.069783 -0.646279   \n",
      "4 -0.110372  0.000000 -0.821356 -0.581720 -0.457143  1.271680 -0.475025   \n",
      "\n",
      "    PTRATIO     LSTAT  CHAS  ...  PTRATIO^2  PTRATIO LSTAT  PTRATIO CHAS  \\\n",
      "0  0.285777 -1.339286   0.0  ...   0.081668      -0.382737           0.0   \n",
      "1  0.569789 -0.446429   0.0  ...   0.324659      -0.254370           0.0   \n",
      "2  0.569789 -0.446429   0.0  ...   0.324659      -0.254370           0.0   \n",
      "3  0.924391 -0.125000   0.0  ...   0.854498      -0.115549           0.0   \n",
      "4  0.924391 -0.125000   0.0  ...   0.854498      -0.115549           0.0   \n",
      "\n",
      "   PTRATIO TAX   LSTAT^2  LSTAT CHAS   LSTAT TAX  CHAS^2  CHAS TAX    TAX^2  \n",
      "0    84.589913  1.793686        -0.0 -396.428571     0.0       0.0  87616.0  \n",
      "1   137.888869  0.199298        -0.0 -108.035714     0.0       0.0  58564.0  \n",
      "2   137.888869  0.199298        -0.0 -108.035714     0.0       0.0  58564.0  \n",
      "3   205.214766  0.015625        -0.0  -27.750000     0.0       0.0  49284.0  \n",
      "4   205.214766  0.015625        -0.0  -27.750000     0.0       0.0  49284.0  \n",
      "\n",
      "[5 rows x 77 columns]\n"
     ]
    }
   ],
   "source": [
    "# Boston Housing Prices - Feature Engineering\n",
    "# -------------------------------------------\n",
    "# Goal: Use feature selection (Mutual Information), create polynomial/interaction features,\n",
    "#       and explore dimensionality reduction (PCA).\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ============================\n",
    "# 1. Mutual Information (Feature Importance)\n",
    "# ============================\n",
    "mi_scores = mutual_info_regression(X_transformed, y)\n",
    "mi_df = pd.DataFrame({\"Feature\": X_transformed_df.columns, \"MI Score\": mi_scores})\n",
    "mi_df = mi_df.sort_values(by=\"MI Score\", ascending=False)\n",
    "print(mi_df)\n",
    "\n",
    "# ðŸ”Ž Observations:\n",
    "# - RM (average rooms) and LSTAT (% lower status) are the strongest predictors.\n",
    "# - TAX shows higher importance than RAD (prefer TAX over RAD).\n",
    "# - CHAS and ZN have relatively low predictive power.\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 2. Polynomial & Interaction Features\n",
    "# ============================\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_transformed)\n",
    "poly_features = poly.get_feature_names_out(X_transformed_df.columns)\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=poly_features)\n",
    "\n",
    "print(X_poly_df.head())\n",
    "\n",
    "# ðŸ”Ž Observations:\n",
    "# - Polynomial features allow capturing nonlinear effects:\n",
    "#   - Squared terms like LSTAT^2 and RM^2.\n",
    "#   - Interaction terms like RM*LSTAT.\n",
    "# - RM and LSTAT become even stronger when squared â†’ consistent with EDA.\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 3. Integrating PolynomialFeatures into Pipeline\n",
    "# ============================\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"log_scale\", log_scale_pipeline, skewed_features),\n",
    "        (\n",
    "            \"scaler\",\n",
    "            RobustScaler(),\n",
    "            [col for col in numeric_features if col not in skewed_features],\n",
    "        ),\n",
    "        (\"cat\", \"passthrough\", categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply transformations\n",
    "X_poly_transformed = pipeline.fit_transform(X)\n",
    "X_poly_df = pd.DataFrame(\n",
    "    X_poly_transformed, columns=poly.get_feature_names_out(X_transformed_df.columns)\n",
    ")\n",
    "print(X_poly_df.head())\n",
    "\n",
    "# ðŸ”Ž Observations:\n",
    "# - Polynomial pipeline integrates preprocessing + feature engineering in one step.\n",
    "# - Expands feature space significantly (quadratic growth).\n",
    "# - Must consider **regularization** later to avoid overfitting.\n",
    "# - I dropped RAD because of its high correlation with TAX. I prefer TAX because he's higher in the MI Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00b858e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (404, 12) | Test shape: (102, 12)\n",
      "\n",
      "Linear Regression Performance (Baseline):\n",
      "Root Mean Squared Error: 3.841\n",
      "Mean Absolute Error: 2.394\n",
      "RÂ² Score: 0.799\n",
      "Cross-Validation RMSE: 5.447 Â± 1.909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_elite/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.681e+03, tolerance: 3.510e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_elite/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.880e+03, tolerance: 3.919e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_elite/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.802e+03, tolerance: 3.307e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_elite/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.830e+03, tolerance: 2.814e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_elite/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.331e+03, tolerance: 3.307e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_elite/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.470e+03, tolerance: 3.481e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Boston Housing Prices - Baseline & Advanced Regression Models\n",
    "# ------------------------------------------------------------\n",
    "# Goal: Train baseline Linear Regression, compare with advanced models,\n",
    "#       and evaluate using RMSE, MAE, and RÂ².\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# ============================\n",
    "# 1. Train-Test Split\n",
    "# ============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"Train shape:\", X_train.shape, \"| Test shape:\", X_test.shape)\n",
    "\n",
    "# ============================\n",
    "# 2. Linear Regression Pipeline\n",
    "# ============================\n",
    "lr_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        (\"selector\", SelectFromModel(Lasso(alpha=0.01))),\n",
    "        (\"model\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train model\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = lr_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nLinear Regression Performance (Baseline):\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.3f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.3f}\")\n",
    "print(f\"RÂ² Score: {r2:.3f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(\n",
    "    lr_pipeline, X, y, cv=5, scoring=\"neg_root_mean_squared_error\"\n",
    ")\n",
    "print(f\"Cross-Validation RMSE: {-cv_scores.mean():.3f} Â± {cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fc11e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_elite/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-09-10 13:51:56,965] A new study created in memory with name: no-name-e1695622-8531-427f-98a4-8e5d53bc6d65\n",
      "[I 2025-09-10 13:52:00,879] Trial 0 finished with value: 3.6541595324990235 and parameters: {'model__n_estimators': 400, 'model__max_depth': 9, 'model__learning_rate': 0.03388276655085706, 'model__subsample': 0.8372464296603439, 'model__colsample_bytree': 0.9980343901951854}. Best is trial 0 with value: 3.6541595324990235.\n",
      "[I 2025-09-10 13:52:02,818] Trial 1 finished with value: 3.6722948363342303 and parameters: {'model__n_estimators': 500, 'model__max_depth': 3, 'model__learning_rate': 0.12464698764195545, 'model__subsample': 0.6078930386419537, 'model__colsample_bytree': 0.7198588278335313}. Best is trial 0 with value: 3.6541595324990235.\n",
      "[I 2025-09-10 13:52:03,481] Trial 2 finished with value: 3.861519082456997 and parameters: {'model__n_estimators': 100, 'model__max_depth': 6, 'model__learning_rate': 0.029218190216873916, 'model__subsample': 0.5112278205115817, 'model__colsample_bytree': 0.9504334959792409}. Best is trial 0 with value: 3.6541595324990235.\n",
      "[I 2025-09-10 13:52:03,872] Trial 3 finished with value: 3.5959620289382976 and parameters: {'model__n_estimators': 100, 'model__max_depth': 3, 'model__learning_rate': 0.08636704854511472, 'model__subsample': 0.7462985812492697, 'model__colsample_bytree': 0.7170521210624411}. Best is trial 3 with value: 3.5959620289382976.\n",
      "[I 2025-09-10 13:52:05,461] Trial 4 finished with value: 3.997852101053266 and parameters: {'model__n_estimators': 300, 'model__max_depth': 8, 'model__learning_rate': 0.18536792661190413, 'model__subsample': 0.991490568128875, 'model__colsample_bytree': 0.7528858419189606}. Best is trial 3 with value: 3.5959620289382976.\n",
      "[I 2025-09-10 13:52:05,929] Trial 5 finished with value: 3.632942249442838 and parameters: {'model__n_estimators': 100, 'model__max_depth': 4, 'model__learning_rate': 0.06404599407017401, 'model__subsample': 0.6179112180542177, 'model__colsample_bytree': 0.6963925881102369}. Best is trial 3 with value: 3.5959620289382976.\n",
      "[I 2025-09-10 13:52:07,960] Trial 6 finished with value: 3.8788951864985868 and parameters: {'model__n_estimators': 200, 'model__max_depth': 9, 'model__learning_rate': 0.15429102154523208, 'model__subsample': 0.6297728901127769, 'model__colsample_bytree': 0.8669149235041954}. Best is trial 3 with value: 3.5959620289382976.\n",
      "[I 2025-09-10 13:52:11,308] Trial 7 finished with value: 3.6613043616721215 and parameters: {'model__n_estimators': 600, 'model__max_depth': 5, 'model__learning_rate': 0.0868161128890777, 'model__subsample': 0.5861907897272135, 'model__colsample_bytree': 0.5968839827877119}. Best is trial 3 with value: 3.5959620289382976.\n",
      "[I 2025-09-10 13:52:13,189] Trial 8 finished with value: 3.679117804037683 and parameters: {'model__n_estimators': 400, 'model__max_depth': 6, 'model__learning_rate': 0.2020900314693412, 'model__subsample': 0.9649438675678068, 'model__colsample_bytree': 0.8966896962283551}. Best is trial 3 with value: 3.5959620289382976.\n",
      "[I 2025-09-10 13:52:14,148] Trial 9 finished with value: 4.638095429640235 and parameters: {'model__n_estimators': 100, 'model__max_depth': 9, 'model__learning_rate': 0.01667353813662102, 'model__subsample': 0.6489877375810185, 'model__colsample_bytree': 0.5268494032619382}. Best is trial 3 with value: 3.5959620289382976.\n",
      "[I 2025-09-10 13:52:17,192] Trial 10 finished with value: 3.5826335425938645 and parameters: {'model__n_estimators': 900, 'model__max_depth': 3, 'model__learning_rate': 0.11262481932346158, 'model__subsample': 0.7833657579934281, 'model__colsample_bytree': 0.8065679467924488}. Best is trial 10 with value: 3.5826335425938645.\n",
      "[I 2025-09-10 13:52:20,207] Trial 11 finished with value: 3.6453723740042285 and parameters: {'model__n_estimators': 900, 'model__max_depth': 3, 'model__learning_rate': 0.2906029698373396, 'model__subsample': 0.7895456145926644, 'model__colsample_bytree': 0.8101167295646532}. Best is trial 10 with value: 3.5826335425938645.\n",
      "[I 2025-09-10 13:52:24,481] Trial 12 finished with value: 3.6140690465084626 and parameters: {'model__n_estimators': 1000, 'model__max_depth': 4, 'model__learning_rate': 0.0857102715736931, 'model__subsample': 0.7306310599666133, 'model__colsample_bytree': 0.6617455362020308}. Best is trial 10 with value: 3.5826335425938645.\n",
      "[I 2025-09-10 13:52:27,501] Trial 13 finished with value: 3.7211108557184445 and parameters: {'model__n_estimators': 700, 'model__max_depth': 4, 'model__learning_rate': 0.1190184392856267, 'model__subsample': 0.8705722063675687, 'model__colsample_bytree': 0.8061532958589411}. Best is trial 10 with value: 3.5826335425938645.\n",
      "[I 2025-09-10 13:52:30,196] Trial 14 finished with value: 3.6076546762438406 and parameters: {'model__n_estimators': 800, 'model__max_depth': 3, 'model__learning_rate': 0.2352360733639217, 'model__subsample': 0.7263655609205499, 'model__colsample_bytree': 0.6303514698118377}. Best is trial 10 with value: 3.5826335425938645.\n",
      "[I 2025-09-10 13:52:33,519] Trial 15 finished with value: 3.845284947489138 and parameters: {'model__n_estimators': 1000, 'model__max_depth': 7, 'model__learning_rate': 0.128216493971474, 'model__subsample': 0.8972122434704471, 'model__colsample_bytree': 0.7669770730068967}. Best is trial 10 with value: 3.5826335425938645.\n",
      "[I 2025-09-10 13:52:37,241] Trial 16 finished with value: 3.5278099063465804 and parameters: {'model__n_estimators': 700, 'model__max_depth': 5, 'model__learning_rate': 0.0706621229217212, 'model__subsample': 0.7896391317613674, 'model__colsample_bytree': 0.8450494351924164}. Best is trial 16 with value: 3.5278099063465804.\n",
      "[I 2025-09-10 13:52:40,284] Trial 17 finished with value: 3.6684137228936335 and parameters: {'model__n_estimators': 800, 'model__max_depth': 5, 'model__learning_rate': 0.16580521103159518, 'model__subsample': 0.8071048875810517, 'model__colsample_bytree': 0.8627085342281068}. Best is trial 16 with value: 3.5278099063465804.\n",
      "[I 2025-09-10 13:52:44,026] Trial 18 finished with value: 3.5464152561097926 and parameters: {'model__n_estimators': 700, 'model__max_depth': 5, 'model__learning_rate': 0.05575752248419347, 'model__subsample': 0.6836724322777927, 'model__colsample_bytree': 0.9136947679953652}. Best is trial 16 with value: 3.5278099063465804.\n",
      "[I 2025-09-10 13:52:48,491] Trial 19 finished with value: 3.6209830851856237 and parameters: {'model__n_estimators': 600, 'model__max_depth': 7, 'model__learning_rate': 0.06085351004931307, 'model__subsample': 0.7077197572936981, 'model__colsample_bytree': 0.9248961831001928}. Best is trial 16 with value: 3.5278099063465804.\n",
      "[I 2025-09-10 13:52:52,235] Trial 20 finished with value: 3.5768253260138207 and parameters: {'model__n_estimators': 700, 'model__max_depth': 5, 'model__learning_rate': 0.05357608144345243, 'model__subsample': 0.6643129601317963, 'model__colsample_bytree': 0.9962157295503296}. Best is trial 16 with value: 3.5278099063465804.\n",
      "[I 2025-09-10 13:52:55,990] Trial 21 finished with value: 3.5951931826329906 and parameters: {'model__n_estimators': 700, 'model__max_depth': 5, 'model__learning_rate': 0.05253926005238145, 'model__subsample': 0.6681460387545709, 'model__colsample_bytree': 0.9971729326747815}. Best is trial 16 with value: 3.5278099063465804.\n",
      "[I 2025-09-10 13:53:01,078] Trial 22 finished with value: 3.5990520604358993 and parameters: {'model__n_estimators': 800, 'model__max_depth': 6, 'model__learning_rate': 0.04521239430282356, 'model__subsample': 0.6884671538745379, 'model__colsample_bytree': 0.9489211650835977}. Best is trial 16 with value: 3.5278099063465804.\n",
      "[I 2025-09-10 13:53:04,824] Trial 23 finished with value: 3.51538934595754 and parameters: {'model__n_estimators': 700, 'model__max_depth': 5, 'model__learning_rate': 0.011131042894767476, 'model__subsample': 0.5451398308031322, 'model__colsample_bytree': 0.863214723461005}. Best is trial 23 with value: 3.51538934595754.\n",
      "[I 2025-09-10 13:53:06,998] Trial 24 finished with value: 3.4518600635587253 and parameters: {'model__n_estimators': 500, 'model__max_depth': 4, 'model__learning_rate': 0.028808971347660223, 'model__subsample': 0.5427252736362448, 'model__colsample_bytree': 0.8486895360981312}. Best is trial 24 with value: 3.4518600635587253.\n",
      "[I 2025-09-10 13:53:09,177] Trial 25 finished with value: 3.4935730011328845 and parameters: {'model__n_estimators': 500, 'model__max_depth': 4, 'model__learning_rate': 0.01634335979999962, 'model__subsample': 0.5249884422814369, 'model__colsample_bytree': 0.8485002947748781}. Best is trial 24 with value: 3.4518600635587253.\n",
      "[I 2025-09-10 13:53:11,381] Trial 26 finished with value: 3.5740702906805453 and parameters: {'model__n_estimators': 500, 'model__max_depth': 4, 'model__learning_rate': 0.011922732935493192, 'model__subsample': 0.5062071703889278, 'model__colsample_bytree': 0.8212381373776277}. Best is trial 24 with value: 3.4518600635587253.\n",
      "[I 2025-09-10 13:53:13,271] Trial 27 finished with value: 3.482106850861436 and parameters: {'model__n_estimators': 400, 'model__max_depth': 4, 'model__learning_rate': 0.020225464936861753, 'model__subsample': 0.5581105721362748, 'model__colsample_bytree': 0.7699255342041449}. Best is trial 24 with value: 3.4518600635587253.\n",
      "[I 2025-09-10 13:53:15,090] Trial 28 finished with value: 3.4362632943610976 and parameters: {'model__n_estimators': 400, 'model__max_depth': 4, 'model__learning_rate': 0.034233099100464154, 'model__subsample': 0.5632214354177936, 'model__colsample_bytree': 0.7850081190578028}. Best is trial 28 with value: 3.4362632943610976.\n",
      "[I 2025-09-10 13:53:18,112] Trial 29 finished with value: 3.6080914174736307 and parameters: {'model__n_estimators': 300, 'model__max_depth': 10, 'model__learning_rate': 0.039749090142763954, 'model__subsample': 0.5618123827986522, 'model__colsample_bytree': 0.7793660892559044}. Best is trial 28 with value: 3.4362632943610976.\n",
      "[I 2025-09-10 13:53:19,828] Trial 30 finished with value: 3.416110291931581 and parameters: {'model__n_estimators': 400, 'model__max_depth': 4, 'model__learning_rate': 0.03482103181075739, 'model__subsample': 0.5823812699421188, 'model__colsample_bytree': 0.6820824341131778}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:21,520] Trial 31 finished with value: 3.4270322681166236 and parameters: {'model__n_estimators': 400, 'model__max_depth': 4, 'model__learning_rate': 0.02913608325889271, 'model__subsample': 0.5687074944154416, 'model__colsample_bytree': 0.7214371379939644}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:22,787] Trial 32 finished with value: 3.484410040844552 and parameters: {'model__n_estimators': 300, 'model__max_depth': 4, 'model__learning_rate': 0.03666350054722639, 'model__subsample': 0.5916178165241273, 'model__colsample_bytree': 0.6910461549778832}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:24,193] Trial 33 finished with value: 3.673008350171039 and parameters: {'model__n_estimators': 400, 'model__max_depth': 3, 'model__learning_rate': 0.09963089304573666, 'model__subsample': 0.5831959564428203, 'model__colsample_bytree': 0.6060242259676581}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:27,275] Trial 34 finished with value: 3.552447404536914 and parameters: {'model__n_estimators': 500, 'model__max_depth': 6, 'model__learning_rate': 0.032097211868509924, 'model__subsample': 0.539206319612838, 'model__colsample_bytree': 0.7272509247717981}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:28,300] Trial 35 finished with value: 3.5171303546822266 and parameters: {'model__n_estimators': 300, 'model__max_depth': 3, 'model__learning_rate': 0.07571193989712192, 'model__subsample': 0.5003322595127053, 'model__colsample_bytree': 0.6746954794403314}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:30,052] Trial 36 finished with value: 3.4204902694244668 and parameters: {'model__n_estimators': 400, 'model__max_depth': 4, 'model__learning_rate': 0.03586757674375611, 'model__subsample': 0.6312579059064657, 'model__colsample_bytree': 0.7344593221252201}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:30,750] Trial 37 finished with value: 3.573568988103746 and parameters: {'model__n_estimators': 200, 'model__max_depth': 3, 'model__learning_rate': 0.1413431641635647, 'model__subsample': 0.628151724466073, 'model__colsample_bytree': 0.7300335026553494}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:32,564] Trial 38 finished with value: 3.5409278103413904 and parameters: {'model__n_estimators': 400, 'model__max_depth': 4, 'model__learning_rate': 0.09626903264625294, 'model__subsample': 0.6172756778439491, 'model__colsample_bytree': 0.6524991085351963}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:34,049] Trial 39 finished with value: 3.748303266535534 and parameters: {'model__n_estimators': 200, 'model__max_depth': 7, 'model__learning_rate': 0.07467203931683017, 'model__subsample': 0.576612645728988, 'model__colsample_bytree': 0.7041374742718446}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:37,292] Trial 40 finished with value: 3.6225667539552804 and parameters: {'model__n_estimators': 400, 'model__max_depth': 8, 'model__learning_rate': 0.039329227099665715, 'model__subsample': 0.6036815402555357, 'model__colsample_bytree': 0.5456227419264912}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:39,433] Trial 41 finished with value: 3.4625235814168023 and parameters: {'model__n_estimators': 500, 'model__max_depth': 4, 'model__learning_rate': 0.025545529703293705, 'model__subsample': 0.5299057347369007, 'model__colsample_bytree': 0.7433433221978194}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:41,956] Trial 42 finished with value: 3.421904247142429 and parameters: {'model__n_estimators': 600, 'model__max_depth': 4, 'model__learning_rate': 0.031452786882268394, 'model__subsample': 0.560053507310442, 'model__colsample_bytree': 0.7857956780189979}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:45,069] Trial 43 finished with value: 3.4682606879207456 and parameters: {'model__n_estimators': 600, 'model__max_depth': 5, 'model__learning_rate': 0.04603442414280376, 'model__subsample': 0.5694131186071016, 'model__colsample_bytree': 0.7854255687492623}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:46,409] Trial 44 finished with value: 3.555297492535387 and parameters: {'model__n_estimators': 400, 'model__max_depth': 3, 'model__learning_rate': 0.06770839637385923, 'model__subsample': 0.645721308474442, 'model__colsample_bytree': 0.7469107000034525}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:48,578] Trial 45 finished with value: 3.952220072000158 and parameters: {'model__n_estimators': 600, 'model__max_depth': 6, 'model__learning_rate': 0.2691375687071959, 'model__subsample': 0.6020746317046465, 'model__colsample_bytree': 0.7149293859571577}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:49,869] Trial 46 finished with value: 3.5294661452190974 and parameters: {'model__n_estimators': 300, 'model__max_depth': 4, 'model__learning_rate': 0.028942485198410053, 'model__subsample': 0.6426105055360082, 'model__colsample_bytree': 0.6820367221674267}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:50,744] Trial 47 finished with value: 3.6008567250068437 and parameters: {'model__n_estimators': 200, 'model__max_depth': 4, 'model__learning_rate': 0.10399169085767088, 'model__subsample': 0.6109392385968668, 'model__colsample_bytree': 0.6529079074299946}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:52,392] Trial 48 finished with value: 3.5931654093476153 and parameters: {'model__n_estimators': 500, 'model__max_depth': 3, 'model__learning_rate': 0.07920907546446118, 'model__subsample': 0.5211154361197333, 'model__colsample_bytree': 0.8240252688584084}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:53,963] Trial 49 finished with value: 3.7182181009464097 and parameters: {'model__n_estimators': 300, 'model__max_depth': 5, 'model__learning_rate': 0.19233092106956007, 'model__subsample': 0.5585522795575015, 'model__colsample_bytree': 0.793064519705469}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:55,337] Trial 50 finished with value: 3.427708591620747 and parameters: {'model__n_estimators': 400, 'model__max_depth': 3, 'model__learning_rate': 0.06228189978184727, 'model__subsample': 0.5906116678921437, 'model__colsample_bytree': 0.7576286213597291}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:56,697] Trial 51 finished with value: 3.466961354226443 and parameters: {'model__n_estimators': 400, 'model__max_depth': 3, 'model__learning_rate': 0.055749215936599525, 'model__subsample': 0.5893931828127267, 'model__colsample_bytree': 0.7562456270125056}. Best is trial 30 with value: 3.416110291931581.\n",
      "[I 2025-09-10 13:53:58,057] Trial 52 finished with value: 3.4065337266252635 and parameters: {'model__n_estimators': 400, 'model__max_depth': 3, 'model__learning_rate': 0.045339369162658225, 'model__subsample': 0.5740073987748237, 'model__colsample_bytree': 0.735274672594527}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:00,023] Trial 53 finished with value: 3.55626346530444 and parameters: {'model__n_estimators': 600, 'model__max_depth': 3, 'model__learning_rate': 0.06253078923519388, 'model__subsample': 0.6333346274875008, 'model__colsample_bytree': 0.708535473866688}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:01,025] Trial 54 finished with value: 3.5375372336625914 and parameters: {'model__n_estimators': 300, 'model__max_depth': 3, 'model__learning_rate': 0.08809471758941984, 'model__subsample': 0.5974950785496241, 'model__colsample_bytree': 0.6324694651729812}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:02,484] Trial 55 finished with value: 3.4533973036428796 and parameters: {'model__n_estimators': 400, 'model__max_depth': 3, 'model__learning_rate': 0.04678532760074971, 'model__subsample': 0.5793507790424579, 'model__colsample_bytree': 0.7326145328921213}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:04,210] Trial 56 finished with value: 3.434907761593658 and parameters: {'model__n_estimators': 500, 'model__max_depth': 3, 'model__learning_rate': 0.02246297665023174, 'model__subsample': 0.6695533449628759, 'model__colsample_bytree': 0.754352998102723}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:05,514] Trial 57 finished with value: 3.5194902287497505 and parameters: {'model__n_estimators': 300, 'model__max_depth': 4, 'model__learning_rate': 0.061593300205696644, 'model__subsample': 0.7534847485309699, 'model__colsample_bytree': 0.7012772089385003}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:08,708] Trial 58 finished with value: 3.53557208293642 and parameters: {'model__n_estimators': 600, 'model__max_depth': 5, 'model__learning_rate': 0.048873950097390095, 'model__subsample': 0.6215649893761255, 'model__colsample_bytree': 0.6658039210710005}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:10,456] Trial 59 finished with value: 3.803967208568396 and parameters: {'model__n_estimators': 400, 'model__max_depth': 4, 'model__learning_rate': 0.23287999171028662, 'model__subsample': 0.9590945113350129, 'model__colsample_bytree': 0.8007188230345861}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:12,161] Trial 60 finished with value: 3.6360601329529123 and parameters: {'model__n_estimators': 500, 'model__max_depth': 3, 'model__learning_rate': 0.010326701604570509, 'model__subsample': 0.5480085779204459, 'model__colsample_bytree': 0.7616132174093153}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:13,869] Trial 61 finished with value: 3.4639271001624494 and parameters: {'model__n_estimators': 500, 'model__max_depth': 3, 'model__learning_rate': 0.021336229211632064, 'model__subsample': 0.6713561252909741, 'model__colsample_bytree': 0.7402638597068916}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:15,574] Trial 62 finished with value: 3.5120367844525022 and parameters: {'model__n_estimators': 500, 'model__max_depth': 3, 'model__learning_rate': 0.024686814043562967, 'model__subsample': 0.6577659490203229, 'model__colsample_bytree': 0.7200120177426491}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:17,319] Trial 63 finished with value: 3.492639933352668 and parameters: {'model__n_estimators': 400, 'model__max_depth': 4, 'model__learning_rate': 0.04346377330988932, 'model__subsample': 0.7000740518644422, 'model__colsample_bytree': 0.7636371011790802}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:19,333] Trial 64 finished with value: 3.4197125973796774 and parameters: {'model__n_estimators': 600, 'model__max_depth': 3, 'model__learning_rate': 0.03388986296848238, 'model__subsample': 0.6382612752123372, 'model__colsample_bytree': 0.6835735293010495}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:21,925] Trial 65 finished with value: 3.4895342030199976 and parameters: {'model__n_estimators': 600, 'model__max_depth': 4, 'model__learning_rate': 0.0376886050043847, 'model__subsample': 0.5187750277910351, 'model__colsample_bytree': 0.684527169531653}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:24,516] Trial 66 finished with value: 3.508173148826647 and parameters: {'model__n_estimators': 600, 'model__max_depth': 4, 'model__learning_rate': 0.05566146424918859, 'model__subsample': 0.6108603307915976, 'model__colsample_bytree': 0.6283045523111959}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:28,256] Trial 67 finished with value: 3.625319484224909 and parameters: {'model__n_estimators': 700, 'model__max_depth': 5, 'model__learning_rate': 0.06658364715564148, 'model__subsample': 0.6353405069699832, 'model__colsample_bytree': 0.7748623414682766}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:29,298] Trial 68 finished with value: 3.4603805517494948 and parameters: {'model__n_estimators': 300, 'model__max_depth': 3, 'model__learning_rate': 0.03133084484153967, 'model__subsample': 0.5721896820665521, 'model__colsample_bytree': 0.8269536247825443}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:31,027] Trial 69 finished with value: 3.6342166730075256 and parameters: {'model__n_estimators': 400, 'model__max_depth': 4, 'model__learning_rate': 0.08311430409601986, 'model__subsample': 0.5418394931872657, 'model__colsample_bytree': 0.6942920042384493}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:32,822] Trial 70 finished with value: 3.5094767857840936 and parameters: {'model__n_estimators': 500, 'model__max_depth': 3, 'model__learning_rate': 0.020547424667882834, 'model__subsample': 0.5875691124887188, 'model__colsample_bytree': 0.5947862308482835}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:34,534] Trial 71 finished with value: 3.468802629875775 and parameters: {'model__n_estimators': 500, 'model__max_depth': 3, 'model__learning_rate': 0.01794240466298231, 'model__subsample': 0.7162167727941222, 'model__colsample_bytree': 0.7498288660632119}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:35,884] Trial 72 finished with value: 3.661873077196823 and parameters: {'model__n_estimators': 400, 'model__max_depth': 3, 'model__learning_rate': 0.1713310954235607, 'model__subsample': 0.6770599667675866, 'model__colsample_bytree': 0.7221875295333002}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:38,003] Trial 73 finished with value: 3.4871602851889514 and parameters: {'model__n_estimators': 500, 'model__max_depth': 4, 'model__learning_rate': 0.03921401106439809, 'model__subsample': 0.6570455190818346, 'model__colsample_bytree': 0.7097102081927881}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:40,004] Trial 74 finished with value: 3.444353885919574 and parameters: {'model__n_estimators': 600, 'model__max_depth': 3, 'model__learning_rate': 0.028199851937127433, 'model__subsample': 0.754147529081571, 'model__colsample_bytree': 0.7430307602442695}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:43,317] Trial 75 finished with value: 3.662761734875523 and parameters: {'model__n_estimators': 400, 'model__max_depth': 8, 'model__learning_rate': 0.051326437161306096, 'model__subsample': 0.6936382039804643, 'model__colsample_bytree': 0.8065527445789133}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:45,462] Trial 76 finished with value: 3.512257832197875 and parameters: {'model__n_estimators': 500, 'model__max_depth': 4, 'model__learning_rate': 0.015190604842478837, 'model__subsample': 0.5518718973320199, 'model__colsample_bytree': 0.667801889100827}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:46,854] Trial 77 finished with value: 3.4171986715170712 and parameters: {'model__n_estimators': 400, 'model__max_depth': 3, 'model__learning_rate': 0.036419418780586674, 'model__subsample': 0.626727776246231, 'model__colsample_bytree': 0.881723335372062}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:47,745] Trial 78 finished with value: 3.488411752122425 and parameters: {'model__n_estimators': 200, 'model__max_depth': 4, 'model__learning_rate': 0.03695105007553043, 'model__subsample': 0.621643870555606, 'model__colsample_bytree': 0.9310176738187099}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:49,341] Trial 79 finished with value: 3.5591218124775246 and parameters: {'model__n_estimators': 300, 'model__max_depth': 5, 'model__learning_rate': 0.07211278585982832, 'model__subsample': 0.5327573161058011, 'model__colsample_bytree': 0.8855135928284373}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:50,701] Trial 80 finished with value: 3.427525435463808 and parameters: {'model__n_estimators': 400, 'model__max_depth': 3, 'model__learning_rate': 0.05813039339705397, 'model__subsample': 0.5962540654318114, 'model__colsample_bytree': 0.9667763941267615}. Best is trial 52 with value: 3.4065337266252635.\n",
      "[I 2025-09-10 13:54:52,057] Trial 81 finished with value: 3.3682314398823374 and parameters: {'model__n_estimators': 400, 'model__max_depth': 3, 'model__learning_rate': 0.04510851703095074, 'model__subsample': 0.6003249231415718, 'model__colsample_bytree': 0.8783673024767273}. Best is trial 81 with value: 3.3682314398823374.\n",
      "[I 2025-09-10 13:54:53,411] Trial 82 finished with value: 3.454160453950267 and parameters: {'model__n_estimators': 400, 'model__max_depth': 3, 'model__learning_rate': 0.04531823219897427, 'model__subsample': 0.5691683696246093, 'model__colsample_bytree': 0.9824784313758095}. Best is trial 81 with value: 3.3682314398823374.\n",
      "[I 2025-09-10 13:54:54,430] Trial 83 finished with value: 3.400603329144605 and parameters: {'model__n_estimators': 300, 'model__max_depth': 3, 'model__learning_rate': 0.031362587458252975, 'model__subsample': 0.6042954308294501, 'model__colsample_bytree': 0.9683109759788142}. Best is trial 81 with value: 3.3682314398823374.\n",
      "[I 2025-09-10 13:54:55,519] Trial 84 finished with value: 3.4318727396011326 and parameters: {'model__n_estimators': 300, 'model__max_depth': 3, 'model__learning_rate': 0.033273316943618146, 'model__subsample': 0.6123035439022998, 'model__colsample_bytree': 0.8938453972285696}. Best is trial 81 with value: 3.3682314398823374.\n",
      "[I 2025-09-10 13:54:56,815] Trial 85 finished with value: 3.432140665350501 and parameters: {'model__n_estimators': 300, 'model__max_depth': 4, 'model__learning_rate': 0.028798465767728536, 'model__subsample': 0.6485770368488325, 'model__colsample_bytree': 0.88003423541448}. Best is trial 81 with value: 3.3682314398823374.\n",
      "[I 2025-09-10 13:54:57,842] Trial 86 finished with value: 3.6719812460604473 and parameters: {'model__n_estimators': 300, 'model__max_depth': 3, 'model__learning_rate': 0.13512112141389787, 'model__subsample': 0.63533803845558, 'model__colsample_bytree': 0.9346003714079835}. Best is trial 81 with value: 3.3682314398823374.\n",
      "[I 2025-09-10 13:54:59,926] Trial 87 finished with value: 4.1340317643033755 and parameters: {'model__n_estimators': 200, 'model__max_depth': 10, 'model__learning_rate': 0.010195351469139233, 'model__subsample': 0.6037599481128642, 'model__colsample_bytree': 0.9741197925781484}. Best is trial 81 with value: 3.3682314398823374.\n",
      "[I 2025-09-10 13:55:00,389] Trial 88 finished with value: 3.658389368862731 and parameters: {'model__n_estimators': 100, 'model__max_depth': 4, 'model__learning_rate': 0.04332349851772829, 'model__subsample': 0.5794118362548486, 'model__colsample_bytree': 0.9158134972892253}. Best is trial 81 with value: 3.3682314398823374.\n",
      "[I 2025-09-10 13:55:01,756] Trial 89 finished with value: 3.5012882835218493 and parameters: {'model__n_estimators': 400, 'model__max_depth': 3, 'model__learning_rate': 0.052850487904084434, 'model__subsample': 0.564202442181842, 'model__colsample_bytree': 0.9484302618658618}. Best is trial 81 with value: 3.3682314398823374.\n",
      "[I 2025-09-10 13:55:05,240] Trial 90 finished with value: 3.4254193313959815 and parameters: {'model__n_estimators': 800, 'model__max_depth': 4, 'model__learning_rate': 0.024719032418938733, 'model__subsample': 0.512933158248857, 'model__colsample_bytree': 0.8377271160053399}. Best is trial 81 with value: 3.3682314398823374.\n",
      "[I 2025-09-10 13:55:09,078] Trial 91 finished with value: 3.426695178198873 and parameters: {'model__n_estimators': 900, 'model__max_depth': 4, 'model__learning_rate': 0.02511895792495815, 'model__subsample': 0.5555328815408469, 'model__colsample_bytree': 0.8439689188341337}. Best is trial 81 with value: 3.3682314398823374.\n",
      "[I 2025-09-10 13:55:12,913] Trial 92 finished with value: 3.4641713280327595 and parameters: {'model__n_estimators': 900, 'model__max_depth': 4, 'model__learning_rate': 0.017353414660027926, 'model__subsample': 0.518182121140184, 'model__colsample_bytree': 0.8373807312940826}. Best is trial 81 with value: 3.3682314398823374.\n",
      "[I 2025-09-10 13:55:17,691] Trial 93 finished with value: 3.4998819600095743 and parameters: {'model__n_estimators': 900, 'model__max_depth': 5, 'model__learning_rate': 0.03544146006924816, 'model__subsample': 0.5087019751777728, 'model__colsample_bytree': 0.8665028586502059}. Best is trial 81 with value: 3.3682314398823374.\n",
      "[I 2025-09-10 13:55:20,379] Trial 94 finished with value: 3.392741455281022 and parameters: {'model__n_estimators': 800, 'model__max_depth': 3, 'model__learning_rate': 0.023522015707787045, 'model__subsample': 0.5533183162775822, 'model__colsample_bytree': 0.9052707218452566}. Best is trial 81 with value: 3.3682314398823374.\n",
      "[I 2025-09-10 13:55:22,706] Trial 95 finished with value: 3.397984652184312 and parameters: {'model__n_estimators': 700, 'model__max_depth': 3, 'model__learning_rate': 0.04305269488254016, 'model__subsample': 0.5331255929933989, 'model__colsample_bytree': 0.8811678040373526}. Best is trial 81 with value: 3.3682314398823374.\n",
      "[I 2025-09-10 13:55:25,365] Trial 96 finished with value: 3.4134162806563024 and parameters: {'model__n_estimators': 800, 'model__max_depth': 3, 'model__learning_rate': 0.05093390860492547, 'model__subsample': 0.5310891799294818, 'model__colsample_bytree': 0.9001791483231516}. Best is trial 81 with value: 3.3682314398823374.\n",
      "[I 2025-09-10 13:55:28,036] Trial 97 finished with value: 3.415608769137531 and parameters: {'model__n_estimators': 800, 'model__max_depth': 3, 'model__learning_rate': 0.047944225368254385, 'model__subsample': 0.5341632659261413, 'model__colsample_bytree': 0.9046707452489269}. Best is trial 81 with value: 3.3682314398823374.\n",
      "[I 2025-09-10 13:55:30,720] Trial 98 finished with value: 3.5353902329529907 and parameters: {'model__n_estimators': 800, 'model__max_depth': 3, 'model__learning_rate': 0.0694136198495214, 'model__subsample': 0.5270042720953079, 'model__colsample_bytree': 0.9000738032397602}. Best is trial 81 with value: 3.3682314398823374.\n",
      "[I 2025-09-10 13:55:33,471] Trial 99 finished with value: 3.4052654451854836 and parameters: {'model__n_estimators': 800, 'model__max_depth': 3, 'model__learning_rate': 0.05031624775229039, 'model__subsample': 0.5361918056408195, 'model__colsample_bytree': 0.8769303585325957}. Best is trial 81 with value: 3.3682314398823374.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGB (prefixed) parameters:\n",
      " {'model__n_estimators': 400, 'model__max_depth': 3, 'model__learning_rate': 0.04510851703095074, 'model__subsample': 0.6003249231415718, 'model__colsample_bytree': 0.8783673024767273}\n",
      "\n",
      "XGBoost Performance (Optuna-tuned):\n",
      "RMSE: 2.392\n",
      "MAE : 1.740\n",
      "R^2 : 0.922\n",
      "CV RMSE: 4.260 Â± 1.164\n",
      "\n",
      "Final Pipeline (re-fit with best params):\n",
      "RMSE: 2.392\n",
      "MAE : 1.740\n",
      "R^2 : 0.922\n"
     ]
    }
   ],
   "source": [
    "# Boston Housing Prices - XGBoost with Optuna Hyperparameter Tuning\n",
    "# -----------------------------------------------------------\n",
    "# Goal: Optimize XGBoost hyperparameters with Optuna,\n",
    "#       evaluate performance, and interpret results.\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "import shap\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from joblib import dump\n",
    "\n",
    "# ---------------------------------------\n",
    "# XGBoost + Optuna (with preprocessing)\n",
    "# ---------------------------------------\n",
    "\n",
    "xgb_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", XGBRegressor(random_state=42, n_jobs=-1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"model__n_estimators\": trial.suggest_int(\n",
    "            \"model__n_estimators\", 100, 1000, step=100\n",
    "        ),\n",
    "        \"model__max_depth\": trial.suggest_int(\"model__max_depth\", 3, 10),\n",
    "        \"model__learning_rate\": trial.suggest_float(\"model__learning_rate\", 0.01, 0.3),\n",
    "        \"model__subsample\": trial.suggest_float(\"model__subsample\", 0.5, 1.0),\n",
    "        \"model__colsample_bytree\": trial.suggest_float(\n",
    "            \"model__colsample_bytree\", 0.5, 1.0\n",
    "        ),\n",
    "    }\n",
    "    xgb_pipeline.set_params(**params)\n",
    "    scores = cross_val_score(\n",
    "        xgb_pipeline, X_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\"\n",
    "    )\n",
    "    return -scores.mean()\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "best_prefixed = study.best_params\n",
    "print(\"Best XGB (prefixed) parameters:\\n\", best_prefixed)\n",
    "\n",
    "# Apply best params to the pipeline directly (prefixed keys)\n",
    "xgb_pipeline.set_params(**best_prefixed)\n",
    "\n",
    "# Train & evaluate on holdout\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_pipeline.predict(X_test)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"\\nXGBoost Performance (Optuna-tuned):\")\n",
    "print(f\"RMSE: {rmse_xgb:.3f}\")\n",
    "print(f\"MAE : {mae_xgb:.3f}\")\n",
    "print(f\"R^2 : {r2_xgb:.3f}\")\n",
    "\n",
    "# Cross-validation on full data\n",
    "cv_scores_xgb = cross_val_score(\n",
    "    xgb_pipeline, X, y, cv=5, scoring=\"neg_root_mean_squared_error\"\n",
    ")\n",
    "print(f\"CV RMSE: {-cv_scores_xgb.mean():.3f} Â± {cv_scores_xgb.std():.3f}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# SHAP Interpretability (global & partial)\n",
    "# ---------------------------------------\n",
    "\n",
    "# Get transformed training features for SHAP\n",
    "X_transformed = xgb_pipeline.named_steps[\"preprocessor\"].transform(X)\n",
    "\n",
    "# Build feature names in the order the ColumnTransformer outputs them\n",
    "feature_names = (\n",
    "    skewed_features\n",
    "    + [col for col in numeric_features if col not in skewed_features]\n",
    "    + categorical_features\n",
    ")\n",
    "\n",
    "xgb_model = xgb_pipeline.named_steps[\"model\"]\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer(X_transformed)\n",
    "\n",
    "# Global summary\n",
    "shap.summary_plot(shap_values, X_transformed, feature_names=feature_names, show=False)\n",
    "plt.title(\"SHAP Summary Plot\")\n",
    "plt.savefig(\"figures/shap_summary_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "# Dependence (example: DIS)\n",
    "shap.dependence_plot(\n",
    "    \"DIS\", shap_values.values, X_transformed, feature_names=feature_names, show=False\n",
    ")\n",
    "plt.savefig(\"figures/shap_dependence_dis.png\")\n",
    "plt.close()\n",
    "\n",
    "# ---------------------------------------\n",
    "# Residuals diagnostic\n",
    "# ---------------------------------------\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(y_pred_xgb, y_test - y_pred_xgb, alpha=0.7)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "plt.xlabel(\"Predicted MEDV\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Predicted Values (XGBoost)\")\n",
    "plt.savefig(\"figures/residuals_vs_predicted.png\")\n",
    "plt.close()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------\n",
    "# Final exportable pipeline (plain XGB params)\n",
    "# ---------------------------------------\n",
    "xgb_params = {\n",
    "    \"n_estimators\": best_prefixed[\"model__n_estimators\"],\n",
    "    \"max_depth\": best_prefixed[\"model__max_depth\"],\n",
    "    \"learning_rate\": best_prefixed[\"model__learning_rate\"],\n",
    "    \"subsample\": best_prefixed[\"model__subsample\"],\n",
    "    \"colsample_bytree\": best_prefixed[\"model__colsample_bytree\"],\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "final_pipeline = make_pipeline(preprocessor, XGBRegressor(**xgb_params))\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "dump(final_pipeline, \"boston_house_price_model.joblib\")\n",
    "\n",
    "y_pred_final = final_pipeline.predict(X_test)\n",
    "print(\"\\nFinal Pipeline (re-fit with best params):\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_final)):.3f}\")\n",
    "print(f\"MAE : {mean_absolute_error(y_test, y_pred_final):.3f}\")\n",
    "print(f\"R^2 : {r2_score(y_test, y_pred_final):.3f}\")\n",
    "\n",
    "# ðŸ”Ž Observations:\n",
    "# - As expected from EDA, RM (rooms) increases prices and LSTAT (lower-status %) decreases them.\n",
    "# - DIS & AGE show moderate effects; DIS interacts with socio-economic context (LSTAT), not distance alone.\n",
    "# - XGBoost is the best fit here (small dataset + nonlinearities), balancing RMSE/MAE with strong R^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "844fb79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neural Network Performance:\n",
      "RMSE: 4.610\n",
      "MAE : 2.941\n",
      "RÂ²  : 0.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/db/5zyr74rj7fn21wgcyltjd3440000gn/T/ipykernel_8262/2359968807.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    }
   ],
   "source": [
    "# Boston Housing Prices - Neural Network (PyTorch)\n",
    "# ------------------------------------------------\n",
    "# Goal: Train a simple feedforward neural network (MLP) for regression\n",
    "#       and evaluate its performance vs. Linear Regression and XGBoost.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ============================\n",
    "# 1. Data Preparation\n",
    "# ============================\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_transformed, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_transformed, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 2. Define Model Architecture\n",
    "# ============================\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "model = MLP(input_dim=X_train_transformed.shape[1])\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ============================\n",
    "# 3. Training Loop (with Early Stopping)\n",
    "# ============================\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "epochs = 200\n",
    "patience = 20\n",
    "best_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = model(X_test_tensor)\n",
    "        val_loss = criterion(y_val_pred, y_test_tensor).item()\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping at epoch\", epoch + 1)\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "# ============================\n",
    "# 4. Evaluation\n",
    "# ============================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_nn = model(X_test_tensor).numpy().flatten()\n",
    "\n",
    "rmse_nn = np.sqrt(mean_squared_error(y_test, y_pred_nn))\n",
    "mae_nn = mean_absolute_error(y_test, y_pred_nn)\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "print(\"\\nNeural Network Performance:\")\n",
    "print(f\"RMSE: {rmse_nn:.3f}\")\n",
    "print(f\"MAE : {mae_nn:.3f}\")\n",
    "print(f\"RÂ²  : {r2_nn:.3f}\")\n",
    "# ðŸ”Ž Observations:\n",
    "# - The neural net underperforms compared to XGBoost and even Linear Regression â€” confirming\n",
    "# that the dataset is too small for deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14a4515e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RÂ²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>3.900</td>\n",
       "      <td>2.482</td>\n",
       "      <td>0.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>2.309</td>\n",
       "      <td>1.724</td>\n",
       "      <td>0.927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network</th>\n",
       "      <td>4.875</td>\n",
       "      <td>3.410</td>\n",
       "      <td>0.676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    RMSE    MAE     RÂ²\n",
       "Linear Regression  3.900  2.482  0.793\n",
       "XGBoost            2.309  1.724  0.927\n",
       "Neural Network     4.875  3.410  0.676"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Boston Housing Prices - Model Comparison\n",
    "# ----------------------------------------\n",
    "# Goal: Compare performance of Linear Regression, XGBoost, and Neural Network.\n",
    "\n",
    "# Collect results\n",
    "results = {\n",
    "    \"Linear Regression\": {\"RMSE\": 3.900, \"MAE\": 2.482, \"RÂ²\": 0.793},\n",
    "    \"XGBoost\": {\"RMSE\": 2.309, \"MAE\": 1.724, \"RÂ²\": 0.927},\n",
    "    \"Neural Network\": {\"RMSE\": 4.875, \"MAE\": 3.410, \"RÂ²\": 0.676},\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "display(results_df)\n",
    "\n",
    "# ============================\n",
    "# Bar Plot Comparison\n",
    "# ============================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# RMSE\n",
    "axes[0].bar(results_df.index, results_df[\"RMSE\"], color=[\"skyblue\", \"orange\", \"green\"])\n",
    "axes[0].set_title(\"RMSE Comparison\")\n",
    "axes[0].set_ylabel(\"RMSE\")\n",
    "\n",
    "# MAE\n",
    "axes[1].bar(results_df.index, results_df[\"MAE\"], color=[\"skyblue\", \"orange\", \"green\"])\n",
    "axes[1].set_title(\"MAE Comparison\")\n",
    "axes[1].set_ylabel(\"MAE\")\n",
    "\n",
    "# RÂ²\n",
    "axes[2].bar(results_df.index, results_df[\"RÂ²\"], color=[\"skyblue\", \"orange\", \"green\"])\n",
    "axes[2].set_title(\"RÂ² Score Comparison\")\n",
    "axes[2].set_ylabel(\"RÂ²\")\n",
    "\n",
    "plt.suptitle(\"Model Performance Comparison\", fontsize=16)\n",
    "plt.savefig(\"figures/model_performance_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "# ðŸ”Ž Observations:\n",
    "# - Linear Regression: Solid baseline (RÂ² â‰ˆ 0.79), but struggles with non-linear relationships.\n",
    "# - XGBoost: Best performer (RMSE â‰ˆ 2.3, RÂ² â‰ˆ 0.93), handling skew, interactions, and non-linear effects very well.\n",
    "# - Neural Network: Underperforms (RÂ² â‰ˆ 0.68), likely due to small dataset size and overfitting risks.\n",
    "\n",
    "# Conclusion:\n",
    "# - Key predictors: RM (rooms â†‘ price), LSTAT (lower-status population â†“ price), plus moderate effects from AGE and DIS.\n",
    "# - Best model: XGBoost, due to its ability to model complex interactions and nonlinearity.\n",
    "# - Lesson learned: For smaller tabular datasets, tree-based methods (like XGBoost) often outperform both linear models and neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_elite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
